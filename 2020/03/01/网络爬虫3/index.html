<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="description" content="">
  <meta name="author" content="Hardy">
  <meta name="keywords" content="">
  <title>网络爬虫3 ~ To be is to do!</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>To be is to do!</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/default.png')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期日, 三月 1日 2020, 3:46 下午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    4.2k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      19 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="第二部分：网络请求"><a href="#第二部分：网络请求" class="headerlink" title="第二部分：网络请求"></a>第二部分：网络请求</h1><p>本部分主要介绍如何通过url链接访问到目标服务器的过程。主要介绍两个库：urllib库和request库。知识结构如下：<br><img src="/images/spider7.png" srcset="/img/loading.gif" alt="" title="spider"> </p>
<h2 id="Part-1-urllib库"><a href="#Part-1-urllib库" class="headerlink" title="Part 1: urllib库"></a>Part 1: urllib库</h2><p>urllib库是python自带的库，功能比较基础，全面。在python3中，所有的urllib库的网络请求功能，都被集中到urllib.request模块下面了。   </p>
<p>1.1 urlopen函数<br>这个函数的作用是，通过url链接，访问服务器。<br>例1：通过urlopen()访问百度主页：      </p>
<pre><code>from urllib import request
url = &#39;http://www.baidu.com&#39;
resp = request.urlopen(url, timeout = 3)
print(type(resp))
print(resp.read())
</code></pre><p>返回的结果如下：    </p>
<pre><code>&lt;class &#39;http.client.HTTPResponse&#39;&gt;
b&#39;&lt;!DOCTYPE html&gt;\n&lt;!--STATUS OK--&gt;\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n
</code></pre><p>可以看到，返回值的类型是http定义下的client下的HTTPResponse类；<br>需要对这个类进行仔细研究，其中的比较重要的属性和方法如下：<br>a. _method = ‘GET’  #说明访问的方法为前面介绍的8种中的get；<br>b. status = 200     #说明返回访问正常<br>c. url = ‘<a href="http://www.baidu.com&#39;#" target="_blank" rel="noopener">http://www.baidu.com&#39;#</a> 说明这个网页最后定位的地址是百度主页；<br>d. fp # 这个属性实际上是可以将这个网页写入本地的buffer，读取这个buffer的方式为’rb’<br>e. headers  # 这个是服务器给网页申请发回的头文件，其子属性headers中存在着一些有用的信息，如：Content-Encoding, Date, Server等内容；</p>
<p>其次是对.read()内容的说明。这里输出的格式是一行（Pycharm），而且也没有出现类似于“百度一下，你就知道”这样的logo，这是因为编码的方式不对。这里将编码模式修改为：utf-8，即可获取下面的结果：    </p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;!--STATUS OK--&gt;                    
&lt;html&gt;
&lt;head&gt;
    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot;&gt;
    &lt;meta content=&quot;always&quot; name=&quot;referrer&quot;&gt;
    &lt;meta name=&quot;theme-color&quot; content=&quot;#2932e1&quot;&gt;
    &lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot; type=&quot;image/x-icon&quot; /&gt;
    &lt;link rel=&quot;search&quot; type=&quot;application/opensearchdescription+xml&quot; href=&quot;/content-search.xml&quot; title=&quot;百度搜索&quot; /&gt;
    &lt;link rel=&quot;icon&quot; sizes=&quot;any&quot; mask href=&quot;//www.baidu.com/img/baidu_85beaf5496f291521eb75ba38eacbd87.svg&quot;&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//s1.bdstatic.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t1.baidu.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t2.baidu.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t3.baidu.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t10.baidu.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t11.baidu.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//t12.baidu.com&quot;/&gt;
    &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//b1.bdstatic.com&quot;/&gt;
    &lt;title&gt;百度一下，你就知道&lt;/title&gt;
    ...
</code></pre><p>为了省略，就只显示部分内容，并将空白删除。可以看到，这里的结果已经能够显示我们获取了百度网页的内容。<br>具体地代码修改为：<code>print(resp.read().decode(&#39;utf-8&#39;))</code>;    </p>
<p>1.2 urlretrieve函数<br>这个函数的作用是，能够将网页上的内容保存下来。<br>例2：利用urlretrieve函数将百度主页下载下来：    </p>
<pre><code>from urllib import request
request.urlretrieve(&#39;http://www.baidu.com/&#39;,&#39;baidu.html&#39;)
</code></pre><p>运行之后，可以看到本地的路径下多了一个baidu.html文件，这个文件就是复制的百度主页的文件。<br>这里首先对urlretrieve这个函数进行解析：<br>urlretrieve(url, filename=None, reporthook=None, data=None)<br>参数url：下载链接地址<br>参数filename：指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）<br>参数reporthook：是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。<br>参数data：指post导服务器的数据，该方法返回一个包含两个元素的(filename, headers) 元组，filename 表示保存到本地的路径，header表示服务器的响应头。   </p>
<p>urlretrieve还能够提示下载进度；<br>例3：利用urlretrieve函数将百度主页下载下来，并显示下载的进度：    </p>
<pre><code>from urllib import request
def cbk(down, block, size):
    &#39;&#39;&#39;回调函数
    down:已经下载的数据块
    block:数据块的大小
    size:远程文件的大小
    &#39;&#39;&#39;
    per = 100.0 * down * block / size
    if per &gt; 100:
        per = 100
    print(&#39;%.2f%%&#39; % per)
url = &#39;http://www.baidu.com&#39;
request.urlretrieve(url,reporthook = cbk)
</code></pre><p>此时，就可以显示下载进度了。     </p>
<p>1.3 urlencode和parse_qs函数<br>发送浏览器请求时，如果有中文或者特殊字符，浏览器会自动给这些字符进行编码。</p>
<blockquote>
<p>备注：这里用到的两个函数是在parse类中。<br>例4：在百度中搜索“爬虫”，打开Chrome浏览器的检查，翻到Network页面，就会发现此时跳转链接如下截图：<br><img src="/images/spider8.PNG" srcset="/img/loading.gif" alt="" title="百度搜索爬虫的跳转链接">    </p>
</blockquote>
<p>那么，为什么是上面那个链接呢？是因为浏览器将输入的爬虫进行了编码。这里使用的编码方式可以通过urlencode进行模拟。<br>例4：用urlencode模拟浏览器搜索爬虫的时候的字符：    </p>
<pre><code>from urllib import parse
kw = {&#39;wd&#39;:&#39;爬虫&#39;}
res = parse.urlencode(kw)    
print(res)
</code></pre><p>这里的输出结果就是“wd=%E7%88%AC%E8%99%AB”了，与浏览器的一致；<br>至于为什么有wd=，就要看百度这个界面的搜索模式了。百度的搜索方法是：<br>在www.baidu.com/s?ite = utf-8&amp;wd=关键字<br>例5：通过编程的方式，返回百度搜索“爬虫”的界面：     </p>
<pre><code>from urllib import request, parse
kw = {&#39;wd&#39;:&#39;爬虫&#39;}
url = &#39;http://www.baidu.com/s?&#39;
url_get = url + &#39;e=UTF-8&amp;&#39; + parse.urlencode(kw)   
request.urlretrieve(url_get,&#39;spider.html&#39;)
</code></pre><p>此时即可观察到返回的html网页。<br>如果想把输出的编码的字符转换为汉字或者能够读懂的字体的话，需要做编码格式的转换，这里就用到了parse_qs函数。<br>例6：将爬虫的编码形式恢复到汉字形式：    </p>
<pre><code>from urllib import parse    
wd = &#39;wd=%E7%88%AC%E8%99%AB&#39;
res = parse.parse_qs(wd)
print(type(res))
print(res)
</code></pre><p>输出的结果为：<code>&lt;class &#39;dict&#39;&gt;</code>和<code>{&#39;wd&#39;: [&#39;爬虫&#39;]}</code>，可以看到，输出的结果是字典的形式，而且输入的字典中value的值得形式是str，现在变成了list。    </p>
<p>1.4 urlparse和urlsplit函数<br>这两个函数的作用是，对url的信息进行分割。    </p>
<blockquote>
<p>备注：这里的两个函数属于parse库。<br>例7：分别利用urlparse和urlsplit函数对例5的url_get进行分割：   </p>
<pre><code>from urllib import parse
url_get = &#39;https://www.baidu.com/s?ie=UTF-8&amp;wd=%E7%88%AC%E8%99%AB&#39;
res_parse = parse.urlparse(url_get)    
res_split = parse.urlsplit(url_get)    
print(res_parse)
print(res_split)
</code></pre><p>输出的结果为：</p>
<pre><code>ParseResult(scheme=&#39;https&#39;, netloc=&#39;www.baidu.com&#39;, path=&#39;/s&#39;, params=&#39;&#39;, query=&#39;ie=UTF-8&amp;wd=%E7%88%AC%E8%99%AB&#39;, fragment=&#39;&#39;)
SplitResult(scheme=&#39;https&#39;, netloc=&#39;www.baidu.com&#39;, path=&#39;/s&#39;, query=&#39;ie=UTF-8&amp;wd=%E7%88%AC%E8%99%AB&#39;, fragment=&#39;&#39;)
</code></pre><p>可以看到，这两个函数都对这个url进行了拆分,对比上一节的内容，可以看出拆分的结果。需要说明的是，这两个函数的区别在于，parse还会对params进行拆分。    </p>
</blockquote>
<p>1.5 实例1： 爬取京东的主页并下载     </p>
<pre><code>from urllib import parse, request
def main(url):
    res = request.urlopen(url)
    print(res.status)
    print(res.read().decode(&#39;utf-8&#39;))
    request.urlretrieve(url, &#39;jd.txt&#39;)
if __name__ == &#39;__main__&#39;:
    url = &#39;https://www.jd.com/&#39;
    main(url)
</code></pre><p>可以在文件中查看下载的资源。这里其实1个例子足够了。</p>
<h2 id="Part-2-requests库"><a href="#Part-2-requests库" class="headerlink" title="Part 2: requests库"></a>Part 2: requests库</h2><p>这个库是需要自己下载安装的，直接pip install requests就可以了。   </p>
<p>2.1 首先介绍requests库的7个方法：<br><img src="/images/spider9.PNG" srcset="/img/loading.gif" alt="" title="requests库的7种方法"><br>其实，这些方法也可以作为request方法的请求方式，如下：<br><img src="/images/spider10.PNG" srcset="/img/loading.gif" alt="" title="requests库的请求方式"><br>这里的**kwargs常用的13个参数如下：<br>a. params:字典或字节序列，作为参数增加到URL中;<br>b. data:字典、字节序列或文件对象，作为request的内容，但是并不放到URL链接里，放在URL链接对应位置作为数据存储；当然也可以把一个字符串赋值给data，该字符串放在URL所对应的位置;<br>c. json::JSON数据，作为request的内容提交，放在服务器的json里面;<br>d. headers:字典，HTTP定制头;<br>e. cookies:字典或CookieJar,request中的cookie;<br>f. auth:元组，支持HTTP认证功能;<br>g. files:字典类型，向某个链接提交传输文件;<br>h. timeout:设定超时时间，单位为s.如果规定时间内服务器不能返回就会产生一个timeout异常;<br>j. proxis:字典类型，设定访问代理服务器，可以增加登录认证，如增加登录用户名和密码。主要用于隐藏爬取网页的源网站，防止爬虫的逆追踪;<br>k. allow_redirects:True/False,默认为True，重定向开关;<br>l. steam:True/False,默认为True，获取内容后立即下载;<br>m. verify:True/False,默认为True，认账SSL证书开关;<br>n. cert:本地SSL证书路径</p>
<p>例8：抓取百度主页：   </p>
<pre><code>import requests
url = &#39;http://www.baidu.com&#39;
res = requests.get(url)
print(type(res))
print(res.status_code)
print(res.text)
</code></pre><p>这里可以返回百度主页的信息。需要关注的是res所属的类别<code>&lt;class &#39;requests.models.Response&#39;&gt;</code>。这个类中需要我们关注如下的5种属性：<br><img src="/images/spider11.PNG" srcset="/img/loading.gif" alt="" title="Response类的5种属性"><br>借助于上述属性，对例8的代码进行优化，如下：    </p>
<pre><code>import requests
def getHtml(url):
    try:
        res = requests.get(url, timeout = 3)
        res.raise_for_status()
        res.encoding = res.apparent_encoding
        return res.text
    except:
        return &#39;产生异常&#39;
if __name__ == &#39;__main__&#39;:
    url = &#39;http://www.baidu.com&#39;
    res = getHtml(url)
    print(res)
</code></pre><p>其中，timeout是访问超时设置，raise_for_status是说返回码如果不对的话，就发出警报；res.encoding = res.apparent_encoding是用来改变text的输出的码制。    </p>
<p>实例2：爬取亚马逊主页<br>按照例8中展示的方法，当我们爬取亚马逊的主页的时候，出现了异常，代码为503，这个代码表示服务器拒绝访问这个页面。<br>这是因为我们发出的请求头不对。我们在代码中加入：<code>print(res.request.headers)</code>可以看到此时的请求头为：    </p>
<pre><code>{&#39;User-Agent&#39;: &#39;python-requests/2.22.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;}
</code></pre><p>其中，可以看到我们访问的爬虫的头部信息为python的requests类，并不是浏览器。因而亚马逊为了反爬虫，对我们的访问提出了拒绝。改进方案：     </p>
<pre><code>import requests
def getHtml(url):
    try:
        kv = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;}
        res = requests.get(url, timeout=3, headers = kv)
        print(res.request.headers)
        res.raise_for_status()
        res.encoding = res.apparent_encoding
        return res.text
    except:
        return &#39;产生异常&#39;
if __name__ == &#39;__main__&#39;:
    url = &#39;https://www.amazon.cn/&#39;
    res = getHtml(url)
    print(res)
</code></pre><p>这里可以看到，我们将输出的请求的头部伪装成为一个正常的浏览器，亚马逊就响应我们的请求了。其中，kv是头部信息，主要修改了User-Agent。里面的字典内的值是取自于Chorme的检查。<br>那么，如何利用request库将文件保存在本地呢？这里利用如下格式：     </p>
<pre><code>with open(path, &#39;wb&#39;) as f:
    f.write(res.content)
</code></pre><p>例9：保存迈克尔杰克逊的一张图片：    </p>
<pre><code>import requests
def getHtml(url, path):
    try:
        kv = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;}
        res = requests.get(url, timeout=3, headers = kv)
        with open(path, &#39;wb&#39;) as f:
            f.write(res.content)
    except:
        return &#39;产生异常&#39;
if __name__ == &#39;__main__&#39;:
    path = &#39;D:/Pycharm/spider/MJ.jpg&#39;
    url = &#39;https://dss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=1135862589,761779521&amp;fm=179&amp;app=42&amp;f=JPEG?w=121&amp;h=140&#39;
    res = getHtml(url, path)
    print(res)
</code></pre><p>这里就可以看到文档下保存的MJ的图片了。</p>
<blockquote>
<p>备注：注意urlretrieve和requests的文件保存方式的区别。    </p>
</blockquote>
<h2 id="Part-3-补充问题"><a href="#Part-3-补充问题" class="headerlink" title="Part 3:补充问题"></a>Part 3:补充问题</h2><p>补充问题1：如何将申请伪装成为浏览器发送的？<br>如果使用urllib库，那么需要如下操作：</p>
<pre><code>from urllib import request
url = &#39;http://www.baidu.com&#39;
kv = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;}
req = request.Request(url, headers = kv)
resp = request.urlopen(req)
print(resp.read().decode(&#39;utf-8&#39;))
</code></pre><p>需要说明的是，这里利用了Request这个类，将浏览器的信息塞给这个类的实例化中的headers部分。<br>如果使用requests库，已经在实例2中展示过了。        </p>
<p>补充问题2：如何设置访问的IP地址？<br>由于爬虫需要快速访问服务器多次，而有的服务器就会对相同IP地址的访问次数进行限制。为了反反爬虫，这里通过设置代理IP的形式进行对抗。<br>常见的IP代理网址：<br>a. 西刺免费代理：<a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a><br>b. 快代理：<a href="http://www.kuaidaili.com" target="_blank" rel="noopener">http://www.kuaidaili.com</a><br>c. 代理云：<a href="http://www.dailiyun.com" target="_blank" rel="noopener">http://www.dailiyun.com</a>     </p>
<blockquote>
<p>备注：免费代理使用的人较多，可能会产生各种问题。<br>类型1：使用urllib库：    </p>
<pre><code>from urllib import request  
url = &#39;http://httpbin.org/ip&#39;  
handler = request.ProxyHanlder(&#39;http&#39;:&#39;218.66.161.88:31769&#39;}
opener = request.build_opener(handler)    
req = request.Request(url)
resp = opener.open(req)    
print(resp.read())
</code></pre><p>这里是有错误的，报值一直如下：    </p>
<pre><code>urllib.error.URLError: &lt;urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。&gt;
</code></pre><p>之后学到改进方案，再回这里整改。    </p>
</blockquote>
<p>补充问题3：如何给访问申请增加cookie？<br>之前介绍过,cookie是用来给服务器提供身份验证的东西。<br>cookie的参数如下：<br>a. name:cookie的名字；<br>b. value:cookie的值；<br>c. expirse:cookie的过期时间；<br>d. path：cookie作用的路径；<br>e. domain:cookie作用的域名；<br>f. secure:是否只有在http协议下起作用。<br>那么，如何利用呢？这里需要使用http.cookiejar库。那么首先介绍这个库。<br>CookieJar类有一些子类，分别是FileCookieJar，MozillaCookieJar，LWPCookieJar。<br>CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。<br>FileCookieJar (filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。<br>MozillaCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与Mozilla浏览器 cookies.txt兼容的FileCookieJar实例。<br>LWPCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与libwww-perl标准的 Set-Cookie3 文件格式兼容的FileCookieJar实例。<br>例10：访问百度主页，并获取主页返回的cookies:    </p>
<pre><code>import urllib
from http.cookiejar import CookieJar
url = &#39;http://www.baidu.com/&#39;
headers = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;}
cookie = CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
resp = opener.open(url)
print(cookie)
</code></pre><p>输出的结果为：    </p>
<pre><code>&lt;CookieJar[&lt;Cookie BAIDUID=99909F7DF63F80ADD7D4FFF9E355E748:FG=1 for .baidu.com/&gt;, &lt;Cookie BIDUPSID=99909F7DF63F80ADA18B24053F86FD2D for .baidu.com/&gt;, &lt;Cookie H_PS_PSSID=1459_21107_30841_30823_26350_22157 for .baidu.com/&gt;, &lt;Cookie PSTM=1582962305 for .baidu.com/&gt;, &lt;Cookie delPer=0 for .baidu.com/&gt;, &lt;Cookie BDSVRTM=0 for www.baidu.com/&gt;, &lt;Cookie BD_HOME=0 for www.baidu.com/&gt;]&gt;
</code></pre><p>这里输出的cookiejar的一个实例。其中的关键字是BAIDUID, BIDUPSID, H_PS_PSSID, PSTM, delPer, BDSVRTM以及BD_HOME。如果给过程添加debug，我们观察一下opener。opener中实际上已经包含了服务器返回给申请的cookie信息，链接为：opener-&gt;handlers-&gt;08-&gt;cookiejar-&gt;_cookies-&gt;’.baidu.com’-&gt;’/‘下面。<br>如果想利用返回的cookie继续访问百度的话，直接用opener即可。     </p>
<pre><code>import urllib
from urllib import parse
from http.cookiejar import CookieJar
url = &#39;http://www.baidu.com/&#39;
headers = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;}
cookie = CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
resp = opener.open(url)
print(cookie)
kw = {&#39;wd&#39;:&#39;爬虫&#39;}
url = &#39;http://www.baidu.com/s?&#39;
url_get = url + &#39;e=UTF-8&amp;&#39; + parse.urlencode(kw) 
resp = opener.open(url_get)
print(cookie)
</code></pre><p>那requests库怎么使用cookie？    </p>
<pre><code>import requests
url = &#39;http://www.baidu.com&#39;
headers = {&#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;}
session = requests.session()
session.post(url, headers = headers)
url_get = &#39;http://www.baidu.com/s?wd=Cokey&#39;
resp = session.get(url_get)
</code></pre><p>在requests库中，将所有的信息存在session中了。debug一下，观察session的类别，会发现cookies的位置如下：session-&gt;cookies-&gt;_cookies-&gt;’baidu.com’-&gt;’/‘。</p>
<p>这一块的内容总结到此。</p>
<blockquote>
<p>备注：requests库是依赖于python自带的urllib库的。这两个库的使用习惯来说，requests库更加好用，更加推荐。     </p>
</blockquote>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2020/03/01/网络爬虫3/';
        this.page.identifier = '/2020/03/01/网络爬虫3/';
      };
      var oldLoad = window.onload;
      window.onload = function () {
        var d = document, s = d.createElement('script');
        s.type = 'text/javascript';
        s.src = '//' + 'Fluid-dev' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      };
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="nofollow noopener noopener">comments
        powered by Disqus.</a></noscript>
  </div>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    


    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  <!-- cnzz Analytics -->
  



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "网络爬虫3&nbsp;",
      ],
      cursorChar: "守拙",
      typeSpeed: 70,
      loop: ture,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>





  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });

      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script src="https://cdn.staticfile.org/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  









</body>
</html>
